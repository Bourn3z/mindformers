model:
  model_config:
    type: Gpt2Config
    use_one_hot_embeddings: False
    num_labels: 1
    dropout_prob: 0.0
    batch_size: 8
    seq_length: 1024
    vocab_size: 50257
    embedding_size: 768
    num_layers: 12
    num_heads: 12
    expand_ratio: 4
    hidden_act: "gelu"
    post_layernorm_residual: False
    hidden_dropout_prob: 0.1
    attention_probs_dropout_prob: 0.1
    max_position_embeddings: 1024
    type_vocab_size: 2
    initializer_range: 0.02
    use_relative_positions: False
    use_past: False
    use_moe: False
    checkpoint_name_or_path: ""
    repetition_penalty: 1
    top_k: 1
    top_p: 0.95
    eos_token_id: 50256
  arch:
    type: GPT2LMHeadModel