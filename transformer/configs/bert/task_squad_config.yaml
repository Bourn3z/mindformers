# Builtin Configurations(DO NOT CHANGE THESE CONFIGURATIONS unless you know exactly what you are doing)
enable_modelarts: False
# Url for modelarts
data_url: ""
train_url: ""
checkpoint_url: ""
# Path for local
data_path: "/cache/data"
output_path: "/cache/train"
load_path: "/cache/checkpoint_path"
enable_profiling: False

# ==============================================================================
description: "run_squad"
do_train: "false"
do_eval: "false"
device_id: 0
epoch_num: 3
num_class: 2
train_data_shuffle: "true"
eval_data_shuffle: "false"
train_batch_size: 32
eval_batch_size: 1
vocab_file_path: ""
eval_json_path: ""
save_finetune_checkpoint_path: "./squad_finetune/ckpt/"
load_pretrain_checkpoint_path: ""
load_finetune_checkpoint_path: ""
train_data_file_path: ""
schema_file_path: ""
# export related
export_batch_size: 1
export_ckpt_file: ''
export_file_name: 'bert_squad'
file_format: 'MINDIR'

arch: 'bert'
model:
  micro_batch_size: 4
  global_batch_size: 4
  seq_length: 1024
  vocab_size: 50304
  embedding_size: 1024
  num_layers: 2
  num_heads: 32
  expand_ratio: 4
  post_layernorm_residual: False
  compute_dtype: fp16
  hidden_act: gelu
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1
  max_position_embeddings: 512
  type_vocab_size: 2
  initializer_range: 0.02
  use_relative_positions: False
  dtype: fp16
  use_past: False
  use_moe: False


seed: 1234
context:
  device_target: 'GPU'
  save_graphs: False
  mode: 0
  graph_kernel_flags: "--disable_expand_ops=Softmax,Dropout --enable_parallel_fusion=false --reduce_fuse_depth=8 --enable_auto_tensor_inplace=true"

parallel_mode: "semi_auto_parallel"

speed_up:
  micro_batch_num: 1
  flatten_weights: False
  fused_kernel: False

moe_config:
  expert_num: 1
  capacity_factor: 1.05
  aux_loss_factor: 0.05
  num_experts_chosen: 1

recompute_config:
  recompute: False
  parallel_optimizer_comm_recompute: False
  mp_comm_recompute: True
  recompute_slice_activation: False

parallel_config:
  data_parallel: 1
  model_parallel: 1
  pipeline_stage: 1
  micro_batch_num: 1
  expert_parallel: 1
  optimizer_shard: False

optimizer: adam

acc_step: 1
full_batch: False
grad_sync_dtype: fp16
data_url: /your/data/path
train_url: ""
checkpoint_url: ""
data_path: "/cache/data"
output_path: "/cache/train"
load_path: "/cache/checkpoint_path"
epoch_size: 1
start_lr: 1e-4
end_lr: 1e-5
warmup_step: 1000
opt_offload: False
sink_size: 10
ckpt_save_dir: ./ckpt
init_loss_scale_value: 65536
scale_factor: 2
scale_window: 1000
