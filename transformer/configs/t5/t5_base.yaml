arch: 't5'
model:
  micro_batch_size: 4
  global_batch_size: 4
  seq_length: 1024
  vocab_size: 50304
  hidden_size: 1024
  intermediate_size: 4096
  hidden_dropout_prob: 0.3
  attention_probs_dropout_prob: 0.3
  max_position_embeddings: 128
  initializer_range: 0.02
  label_smoothing: 0.1
  beam_width: 4
  max_decode_length: 80
  length_penalty_weight: 1.0
  hidden_act: relu
  num_hidden_layers: 2
  num_heads: 32
  compute_dtype: fp16

seed: 1234
context:
  device_target: 'GPU'
  save_graphs: False
  mode: 0
  graph_kernel_flags: "--disable_expand_ops=Softmax,Dropout --enable_parallel_fusion=true --reduce_fuse_depth=8 --enable_auto_tensor_inplace=true"

parallel_mode: "semi_auto_parallel"
enable_parallel_optimizer: False

speed_up:
  micro_batch_num: 1
  flatten_weights: False
  fused_kernel: False

moe_config:
  expert_num: 1
  capacity_factor: 1.05
  aux_loss_factor: 0.05
  num_experts_chosen: 1

recompute_config:
  recompute: False
  parallel_optimizer_comm_recompute: False
  mp_comm_recompute: True
  recompute_slice_activation: False

parallel_config:
  data_parallel: 1
  model_parallel: 1
  pipeline_stage: 1
  micro_batch_num: 1
  expert_parallel: 1

optimizer: adam

acc_step: 1
grad_sync_dtype: fp16
data_url: /your/data/path
epoch_size: 1

# Learning Rate
learning_rate: 2.0
warmup_step: 8000
start_decay_step: 16000
min_lr: 0.0


opt_offload: False
sink_size: 10
ckpt_save_dir: ./ckpt
init_loss_scale_value: 65536
scale_factor: 2
scale_window: 1000
