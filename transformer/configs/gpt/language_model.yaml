# Builtin Configurations(DO NOT CHANGE THESE CONFIGURATIONS unless you know exactly what you are doing)
enable_modelarts: False
# Url for modelarts
# Path for local
enable_profiling: False

# ==============================================================================
description: "run_classifier"
assessment_method: "Accuracy"
do_train: "false"
do_eval: "false"
eval_type: "zero-shot"
device_id: 0
epoch_num: 3
num_class: 2
train_data_shuffle: "true"
eval_data_shuffle: "false"
save_finetune_checkpoint_path: "./classifier_finetune/ckpt/"
load_pretrain_checkpoint_path: ""
load_finetune_checkpoint_path: ""
train_data_path: ""
eval_data_path: ""
schema_file_path: ""
# export related
export_file_name: 'gpt_classifier'
file_format: 'MINDIR'
metric_method: 'PPL'

arch: 'gpt'
model:
  train_batch_size: 1
  eval_batch_size: 1  
  seq_length: 1024
  vocab_size: 50304
  hidden_size: 1024
  num_layers: 2
  num_heads: 32
  expand_ratio: 4
  post_layernorm_residual: False
  dropout_rate: 0.1
  compute_dtype: fp16
  layernorm_dtype: fp32
  softmax_dtype: fp32


seed: 1234
context:
  device_target: 'GPU'
  save_graphs: False
  mode: 0
  graph_kernel_flags: "--disable_expand_ops=Softmax,Dropout --enable_parallel_fusion=true --reduce_fuse_depth=8 --enable_auto_tensor_inplace=true"

parallel_mode: "semi_auto_parallel"

speed_up:
  micro_batch_num: 1
  flatten_weights: False
  fused_kernel: False

moe_config:
  expert_num: 1
  capacity_factor: 1.05
  aux_loss_factor: 0.05
  num_experts_chosen: 1

recompute_config:
  recompute: False
  parallel_optimizer_comm_recompute: False
  mp_comm_recompute: True
  recompute_slice_activation: False

parallel_config:
  data_parallel: 1
  model_parallel: 1
  pipeline_stage: 1
  micro_batch_num: 1
  expert_parallel: 1
  optimizer_shard: False

optimizer: adam

acc_step: 1
full_batch: False
grad_sync_dtype: fp16
data_url: /your/data/path
train_url: ""
checkpoint_url: ""
data_path: "/cache/data"
output_path: "/cache/train"
load_path: "/cache/checkpoint_path"
epoch_size: 1
start_lr: 1e-4
end_lr: 1e-6
warmup_step: 1000
opt_offload: False
sink_size: 10
ckpt_save_dir: ./ckpt
init_loss_scale_value: 65536
scale_factor: 2
scale_window: 1000
