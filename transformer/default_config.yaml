# Builtin Configurations(DO NOT CHANGE THESE CONFIGURATIONS unless you know exactly what you are doing)
enable_modelarts: False
data_url: ""
train_url: ""
checkpoint_url: ""
data_path: "/cache/data"
output_path: "/cache/train"
load_path: "/cache/checkpoint_path"
checkpoint_path: ''
device_target: Ascend
enable_profiling: False

# ==============================================================================
# learning rate
start_lr: 0.0001
end_lr: 0.000001
warmup_step: 10000
weight_decay: 0.01  # 1e-5
decay_filter: ['layernorm', 'bias']
# config/cfg edict
transformer_network: 'large'
init_loss_scale_value: 65536
scale_factor: 2
scale_window: 2000
optimizer: 'Adam'
optimizer_adam_beta2: 0.997
#lr_schedule: edict({'learning_rate': 2.0, 'warmup_steps': 8000, 'start_decay_step': 16000, 'min_lr': 0.0,})

sink_size: 100
# transformer_net_cfg
global_batch_size: 128
batch_size: 128
seq_length: 128
vocab_size: 30522
hidden_size: 768
num_hidden_layers: 12
num_attention_heads: 12
intermediate_size: 3072
hidden_act: "gelu"
hidden_dropout_prob: 0.1
attention_probs_dropout_prob: 0.1
max_position_embeddings: 128
initializer_range: 0.02
label_smoothing: 0.1
dtype: float16
compute_dtype: float16

#eval_config/cfg edict
data_file: '/cache/data'
data_file_name: 'newstest2014-l128-mindrecord'
model_file: './transformer/transformer_trained.ckpt'
output_file: './output_eval.txt'

# transformer_net_cfg
batch_size_ev: 1
hidden_dropout_prob_ev: 0.0
attention_probs_dropout_prob_ev: 0.0
beam_width: 4
max_decode_length: 80
length_penalty_weight: 1.0
micro_batch_num: 1

# ==============================================================================
# train.py / Argparse init.
distribute: "false"
data_from_obs: "false"
opt_offload: "false"
flatten_weights: "false"
epoch_size: 20
device_id: 0
device_num: 1
enable_lossscale: "true"
do_shuffle: "true"
enable_save_ckpt: "true"
save_checkpoint_steps: 10000
save_checkpoint_num: 30
save_checkpoint_path: "./"
bucket_boundaries:
  - 16
  - 32
  - 48
  - 64
  - 128
accumulation_steps: 1

# export.py /eval_config - transformer export
file_name: "transformer"
file_format: 'AIR'

#'postprocess / from eval_config'
result_dir: "./result_Files"

#'preprocess / from eval_config'
result_path: "./preprocess_Result/"

# src/process_output.py "recore nbest with smoothed sentence-level bleu."
vocab_file: ""

# create_data.py
input_file: ''
num_splits: 16
clip_to_max_len: False
max_seq_length: 128
bucket: [16, 32, 48, 64, 128]

# ckpt setting
ckpt_save_dir: './'

# TransformerOpParallelConfig
parallel_mode: data_parallel
data_parallel: 1
model_parallel: 1
pipeline_stage: 1
micro_batch_num: 1
recompute: False
parallel_comm_recompute: False
mp_comm_recompute: False
recompute_slice_activation: False
optimizer_shard: False
gradient_aggregation_group: 1
vocab_emb_dp: True
expert_parallel_num: 1
expert_num: 1
per_token_num_experts_chosen: 1
acc_step: 1


---
# Config description for each option
enable_modelarts: 'Whether training on modelarts, default: False'
data_url: 'Dataset url for obs'
train_url: 'Training output url for obs'
data_path: 'Dataset path for local, it is better to use absolute path'
output_path: 'Training output path for local'
ann_file: 'Ann file, default is val.json.'

device_target: "device where the code will be implemented, default is Ascend"
checkpoint_path: "Checkpoint file path"
data_file: '/your/path/evaluation.mindrecord'
model_file: '/your/path/checkpoint_file'
output_file: './output_eval.txt'

distribute: "Run distribute, default is false."
data_from_obs: "Download data from obs, default is false."
opt_offload: "Use CPU optimizer, default is false."
flatten_weights: "Flatten weights and merge optimizer ops, default is false."
epoch_size: "Epoch size, default is 52."
device_id: "Device id, default is 0."
device_num: "Use device nums, default is 1."
enable_lossscale: "Use lossscale or not, default is true."
do_shuffle: "Enable shuffle for dataset, default is true."
enable_save_ckpt: "Enable save checkpoint, default is true."
save_checkpoint_steps: "Save checkpoint steps, default is 2500."
save_checkpoint_num: "Save checkpoint numbers, default is 30."
save_checkpoint_path: "Save checkpoint file path"
bucket_boundaries: "sequence length for different bucket"
accumulation_steps: "Gradient accumulation steps, default is 1."

file_name: "output file name."
file_format: 'file format'
result_dir: "./result_Files"
result_path: "./preprocess_Result/"
vocab_file: "vocab file path."
input_file: 'Input raw text file (or comma-separated list of files).'
num_splits: 'The MindRecord file will be split into the number of partition.'
clip_to_max_len: 'clip sequences to maximum sequence length.'
max_seq_length: 'Maximum sequence length.'
bucket: 'bucket sequence length'


# ParallelConfig Setting
parallel_mode: "The parallel running mode"
data_parallel: "the data parallel way"
model_parallel: "the model parallel way"
pipeline_stage: "the pipeline parallel way"
micro_batch_num: "micro batch num for pipeline training"
recompute: "Open recompute or not"
optimizer_shard: "Parallel Optimizer"
gradient_aggregation_group: "group numbers"
vocab_emb_dp: "WordEmbedding"
expert_parallel_num: "expert_parallel_num"
expert_num: "The total expert number"
per_token_num_experts_chosen: "per_token_num_experts_chosen"
acc_step: "An int. Used for controlling the steps of gradient accumulation. If it's 1, the gradient accumulation will be disabled. If it's greater than 1, The model will accumulate acc_step-1 steps and then be updated in step acc_step. Default 1"

---
device_target: ["Ascend", "GPU", "CPU"]
file_format: ["AIR", "ONNX", "MINDIR"]
distribute: ['true', 'false']
data_from_obs: ['true', 'false']
opt_offload: ['true', 'false']
flatten_weights: ['true', 'false']
enable_lossscale: ['true', 'false']
do_shuffle: ['true', 'false']
enable_save_ckpt: ['true', 'false']
